# Multi-head-causal-attention-mechanism

ğˆ ğœğ«ğğšğ­ğğ ğš ğœğ¨ğ¦ğ©ğ¥ğğ­ğ ğ°ğ¨ğ«ğ¤ğŸğ¥ğ¨ğ° ğ¨ğŸ ğŒğ®ğ¥ğ­ğ¢-ğ‡ğğšğ ğ‚ğšğ®ğ¬ğšğ¥ ğ€ğ­ğ­ğğ§ğ­ğ¢ğ¨ğ§, ğ­ğ¡ğ ğœğ¨ğ«ğ ğ¦ğğœğ¡ğšğ§ğ¢ğ¬ğ¦ ğ®ğ¬ğğ ğ¢ğ§ ğ†ğğ“-ğŸ ğŸğ¨ğ« ğ©ğ«ğ¨ğœğğ¬ğ¬ğ¢ğ§ğ  ğ¬ğğªğ®ğğ§ğœğğ¬

High-level steps

1.Inputs: (1, 3, 6)

2.Weight matrices for projections: W_q, W_k, W_v (shapes based on embed_dim) â€” e.g., (6, 6) each

3.linear projections â†’ produce Q, K, V: (1, 3, 6)

Reshape to group by tokens & heads: (1, 3, num_heads, head_dim) â†’ e.g., (1, 3, 2, 3)

Transpose to group by heads: (1, num_heads, seq_len, head_dim) â†’ e.g., (1, 2, 3, 3)

4.Compute attention scores: scores = Q @ Káµ€ (transpose last two dims)

5.Apply causal mask (prevent attending to future tokens) based on context size

6.Scale and normalize (softmax) the attention scores

7.Multiply attention weights by V; then reverse the reshape/transpose to reconstruct token grouping: (1, 3, num_heads, head_dim)

8.Concatenate heads â†’ (1, 3, embed_dim) (e.g., (1, 3, 6))

9.Optional final linear projection to ensure input/output dimension match

10.Output: (1, 3, 6)
